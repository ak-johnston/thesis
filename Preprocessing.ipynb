{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: /Users/aimeejohnston/Desktop/beatLab/fnproject/fnve/bin/pip: bad interpreter: /Users/aimeejohnston/Desktop/beatLab/gammapotamus/fnproject/fnve/bin/python3.11: no such file or directory\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.11.1)\n",
      "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scipy) (1.24.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "zsh:1: /Users/aimeejohnston/Desktop/beatLab/fnproject/fnve/bin/pip: bad interpreter: /Users/aimeejohnston/Desktop/beatLab/gammapotamus/fnproject/fnve/bin/python3.11: no such file or directory\n",
      "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (3.8.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (4.42.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (10.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install scipy\n",
    "!pip install matplotlib\n",
    "\n",
    "# Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.gridspec as gridspec\n",
    "import scipy\n",
    "import scipy.io as io\n",
    "import scipy.signal as signal\n",
    "from scipy.signal import decimate \n",
    "from scipy.signal import cwt\n",
    "import scipy.linalg as linalg\n",
    "from scipy.stats import zscore\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seiji's Preprocessing Functions\n",
    "def labeling(x):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    x = 1 dimensional boolean array\n",
    "\n",
    "    Output:\n",
    "    x = 1 dimensional labeled array\n",
    "\n",
    "    Description:\n",
    "    This function takes a boolean array and applies unique numerical labeling by clusters of True elements.\n",
    "    \"\"\"\n",
    "    # Initialize starting point\n",
    "    label = 1\n",
    "\n",
    "    # Apply unique ID to each cluster\n",
    "    for idx, i in enumerate(x):\n",
    "        if i == True:\n",
    "            x[idx] = label\n",
    "        elif i == False:\n",
    "            if x[idx - 1] != False:\n",
    "                label += 1\n",
    "\n",
    "    # Return the new array\n",
    "    return x\n",
    "\n",
    "def compAnly(Track, fs):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    Track = Contains information on the session pertaining to the track\n",
    "    fs = sampling frequency\n",
    "\n",
    "    Output:\n",
    "    lapID = Data stored in the format listed below\n",
    "\n",
    "    lapID:\n",
    "    Column 0: Trial Number\n",
    "    Column 1: Port (1/2/3)\n",
    "    Column 2: Correct (0/1) Note: This used to be (1/2), but I looked at the\n",
    "    data and it was (0/1)\n",
    "    Column 3: first approach/port/last departure/other (1/2/3/4)\n",
    "\n",
    "    Description:\n",
    "    This function extracts all the necessary metadata for the rat's position\n",
    "    that will be used in future data processing.\n",
    "    \"\"\"\n",
    "\n",
    "    from scipy.signal import decimate\n",
    "    import numpy as np\n",
    "    from statistics import median\n",
    "\n",
    "    # Create the original matrix, calling known information\n",
    "    lapID = np.array([ np.squeeze(Track[\"lapID\"][0][0])[::int(1250/fs)] ])\n",
    "    lapID = np.append(lapID, [ np.squeeze(Track[\"mazeSect\"][0][0])[::int(1250/fs)] ], axis = 0)\n",
    "    lapID = np.append(lapID, [ np.squeeze(Track[\"corrChoice\"][0][0])[::int(1250/fs)] ], axis = 0)\n",
    "    lapID = np.append(lapID, np.zeros((1, len(lapID[0]))), axis = 0)\n",
    "\n",
    "    # Transpose to make lapID of appropriate dimensions\n",
    "    lapID = lapID.T\n",
    "\n",
    "    # Filter values and construct column 3\n",
    "    lapID[np.in1d(lapID[:,1], np.array(range(4, 10)), invert = True), 1] = 0\n",
    "    lapID[lapID[:, 1] > 6, 3] = 2\n",
    "    lapID[lapID[:, 1] > 0, 1] = (lapID[lapID[:,1] > 0, 1] - 1) % 3 + 1\n",
    "    lapID[lapID[:, 1] == 0, :] = 0\n",
    "    # Identify samples corresponding to the first period when the animal enters the maze section\n",
    "    for i in range(1, int(max(lapID[:, 0]) + 1)):\n",
    "        r = labeling( 0 + np.logical_and((lapID[:, 0] == i), (lapID[:, 3] == 2)) )\n",
    "        inds = np.where(lapID[:, 0] == i)[0]\n",
    "        r1 = labeling( 0 + np.logical_and((lapID[:, 0] == i), (lapID[:, 3] == 0)) )\n",
    "        lapID[r1 == max(np.multiply(r1, np.roll(r == 1,-1, axis = 0))), 3] = 1\n",
    "        lapID[inds[inds > np.where(r > 0)[0][-1]], 3] = 3\n",
    "        lapID[inds[lapID[inds, 3] == 0], 3] = 5\n",
    "        lapID[r > 1, 3] = 4\n",
    "\n",
    "    # Return structured data\n",
    "    return lapID\n",
    "\n",
    "def trainTestSplit(lapID, numTrain):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    lapID = lapID matrix\n",
    "    numTrain = Some arbitrarily large number\n",
    "\n",
    "    Output:\n",
    "    trainInds = the index breakdown for the training dataset.\n",
    "\n",
    "    Description:\n",
    "    This is a specialized train/test split that generates a permutation (fold)\n",
    "    for the number of prespecified cross validations. This function ensures\n",
    "    that there is an even quantity of data for each output category within each\n",
    "    cross validation fold.\n",
    "    \"\"\"\n",
    "\n",
    "    import random\n",
    "    import numpy as np\n",
    "\n",
    "    # Initialize variable dimensions\n",
    "    numCategories = max(lapID[:, 2])\n",
    "    trainInds = np.zeros( (max(lapID[:, 1]), numTrain, numCategories) )\n",
    "    minTrain = numTrain\n",
    "\n",
    "    # Add a permutation of each output category for each cross validation fold\n",
    "    for i in range(max(lapID[:, 1])):  # For each cross validation fold...\n",
    "        for c in range(numCategories): # And for each port (output)...\n",
    "            # Indices of values\n",
    "            l = np.where(np.logical_and(lapID[:, 2] == c + 1, lapID[:, 1] != i + 1))[0]\n",
    "            minTrain = min(minTrain, len(l))\n",
    "            trainInds[i, 0:minTrain, c] = l[np.random.permutation(len(l))[0:minTrain]]\n",
    "\n",
    "    # Reduce array to 2 dimensions in the form of (indices, CrossVal Folds)\n",
    "    trainInds = trainInds[:, 0:minTrain, :]\n",
    "    trainInds = np.concatenate((trainInds[:, :, 0], trainInds[:, :, 1], trainInds[:, :, 2]), axis = 1).T\n",
    "\n",
    "    # Return an integer array\n",
    "    return trainInds.astype(int), minTrain\n",
    "\n",
    "# Formatting raw files\n",
    "def lustigDataFormatting(mat_file, lfp_file, dec=50):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    mat_file = metadata file of type .mat\n",
    "    lfp_file = raw lfp data file of type .lfp\n",
    "    dec = decimation value\n",
    "\n",
    "    Output:\n",
    "    X = formatted lfp data\n",
    "    lapID = formatted metadata\n",
    "    sp = 2D histogram of spikes between time and channels\n",
    "\n",
    "    Description:\n",
    "    This function takes raw metadata and lfp files and returns a formatted\n",
    "    metadata and lfp variables to undergo further formatting.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract the needed data from the LFP.mat file.\n",
    "    mat = io.loadmat(mat_file, variable_names = ['Track', 'xml', 'Spike', 'Clu'])\n",
    "    Track = mat['Track']\n",
    "    xml = mat['xml']\n",
    "    Spike = mat['Spike']\n",
    "    Clu = mat['Clu']\n",
    "\n",
    "    # Extract the information from the bit .lfp data file and reshape to the appropriate matrix.\n",
    "    a = np.memmap(lfp_file, 'int16')\n",
    "    a = a.reshape((-1, xml['nChannels'][0][0][0][0]))\n",
    "\n",
    "    # Bin both dimensions using histogram2d.\n",
    "    sp, xedges, yedges = np.histogram2d(np.squeeze(Spike['res'][0][0]/dec), np.squeeze(Spike['totclu'][0][0]) - 1, bins = (np.arange(np.ceil(max(Spike['res'][0][0])/dec) + 1), np.arange(max(Spike['totclu'][0][0]) + 1)) )\n",
    "    sp = np.uint8(sp)\n",
    "\n",
    "    # Clip the dimensions to ensure the appropriate number of electrodes. Use multiple [0]'s to access the desired data.\n",
    "    if np.size(a, 1) > 256:\n",
    "        a = a[:, :255]\n",
    "        sp[:, Clu['shank'][0][0][0] > 32] = []\n",
    "    else:\n",
    "        a = a[:, :192]\n",
    "        sp[:, Clu['shank'][0][0][0] > 24] = []\n",
    "\n",
    "    # Create the data matrix on which preprocessing will take place.\n",
    "    X = np.zeros((np.size(a, 1), math.ceil(np.size(a, 0)/dec)))\n",
    "    for j in range(np.size(a, 1)):\n",
    "        doubleArray = np.asarray(a[:, j], dtype = np.float64, order ='C')\n",
    "        X[j,:] = decimate(doubleArray, dec)\n",
    "\n",
    "    # Construct lapID metadata using compAnly\n",
    "    lapID = compAnly(Track, 1250/dec)\n",
    "    return X, lapID, sp\n",
    "\n",
    "# Preprocessing, drawing from brianPyNet.m\n",
    "def brianPyNet(data, width, dec = 8, numCrossVals = 5, numSessions = 1):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    data = a dictionary containing X, lapID, and sp\n",
    "    dec = decimation value\n",
    "    numCrossVals = number of cross validations to be generated\n",
    "    numSessions = number of sessions of available data\n",
    "\n",
    "    Output:\n",
    "    save file = generates unique save files for each session and running/stillness\n",
    "    combination. Format: .npy\n",
    "\n",
    "    save file contains:\n",
    "    X = input lfp data\n",
    "    lapID = output information\n",
    "    trainInds = indices for cross validations, used for the train-test split\n",
    "    sp = 2D histogram of spikes between time and channels\n",
    "\n",
    "    Description:\n",
    "    This function constructs a complete preprocessed input (X) and output (lapID) for\n",
    "    FieldNet, and constructs indices for the train-test split (trainInds). This\n",
    "    function also splits the data into the running and stillness datasets used in\n",
    "    the manuscript. Files are saved as dictionaries in .npy format.\n",
    "    \"\"\"\n",
    "\n",
    "    import numpy as np\n",
    "    import math\n",
    "    import copy\n",
    "\n",
    "    # Testing parameters\n",
    "    # gpSizes = np.zeros((numCrossVals, 2))\n",
    "\n",
    "    # Construct multiple cross validations\n",
    "    for i in range(numSessions):\n",
    "\n",
    "        # Separate running and stillness\n",
    "        LAPID = data['lapID']\n",
    "        for j in range(1, 3): #4?\n",
    "\n",
    "            # Construct output lapID and lfp data for each output\n",
    "            inds = LAPID[:, 3] == j\n",
    "            if j == 2:\n",
    "                inds = np.logical_and(inds, LAPID[:, 2] == 1) # use & alias\n",
    "            lapID = LAPID[inds, 0:2]\n",
    "            lapID = np.insert(lapID, 1, 0, axis = 1)\n",
    "            lapID = np.append(lapID, np.where(inds)[0][:, np.newaxis], 1).astype(int)\n",
    "            \n",
    "            Xfull = data['X']\n",
    "            X = Xfull[inds, :]\n",
    "\n",
    "\n",
    "            # Construct array of training indices\n",
    "            trs = np.unique(lapID[:, 0])\n",
    "            trs = trs[np.random.permutation(len(trs))]\n",
    "            trainInds = np.array([])\n",
    "\n",
    "            # Loop through 100 different fold assignments to find the largest one\n",
    "            for k in range(100):\n",
    "                # Assign a cross validation fold to each represented trial\n",
    "                for l in range(len(trs)):\n",
    "                    trialInds = lapID[:, 0] == trs[l]\n",
    "                    if sum(trialInds) != 0:\n",
    "                        lapID[trialInds, 1] = math.ceil(np.random.rand(1)[0] * numCrossVals)\n",
    "                temp, _ = trainTestSplit(lapID, 20000)\n",
    "                if temp.shape[0] > trainInds.shape[0]:\n",
    "                    trainInds = copy.deepcopy(temp)\n",
    "                    lapBest = copy.deepcopy(lapID)\n",
    "                    # gpSizes[l, j - 1] = trainInds.shape[0] ## An internal check for sizes between loops\n",
    "            lapID = lapBest\n",
    "\n",
    "            # Save a .npy file for each session and running/stillness combo\n",
    "            data_output = {'X': X, 'lapID': lapID, 'trainInds': trainInds, 'sp': data['sp']}\n",
    "            np.save(f\"pyDat{width}{i}{j-1}.npy\", data_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter functions!\n",
    "\n",
    "def filtHigh(sig, fs, fH, order = 2):\n",
    "    #Highpass filter for full broadband data\n",
    "\n",
    "    # Construct the numerator and denominator for the filtfilt function\n",
    "    b, a = signal.sos2tf(signal.butter(order, fH, btype='highpass', fs = fs, analog=False, output = 'sos'))\n",
    "\n",
    "    # Apply the highpass filter\n",
    "    sig = signal.filtfilt(b, a, sig.T).T\n",
    "\n",
    "    return sig\n",
    "\n",
    "def filter(matrix, range, btype='bp', fs = 1250/8, order = 3):\n",
    "    #Bandpass filter for specific broadbands\n",
    "    filter = signal.butter(order, range, btype, output = 'sos', fs = fs)\n",
    "    filt_X = signal.sosfiltfilt(filter, matrix, axis= 1)\n",
    "    return filt_X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify target files for data drudging/formatting\n",
    "mat_file = \"rec01_BehavElectrDataLFP.mat\" # File name: \"rec01_BehavElectrDataLFP.mat\"\n",
    "lfp_file = \"rec01.lfp\" # File name: \"rec01.lfp\"\n",
    "\n",
    "# Generate dictionary containing X, lapID, and sp.\n",
    "X, lapID, sp = lustigDataFormatting(mat_file, lfp_file, dec = 8)\n",
    "\n",
    "#Filter data into a given bandwidth\n",
    "X_full = filtHigh(X.T, 1250/8, 2)\n",
    "theta_X = filter(X_full, range= [6,12], btype= 'bp',order= 2) \n",
    "harm_X = filter(X_full, range= [14,20], btype= 'bp',order= 2) \n",
    "\n",
    "H_X = signal.hilbert(X_full, axis= 0)\n",
    "Htheta_X = signal.hilbert(theta_X, axis= 0)\n",
    "Hharm_X = signal.hilbert(harm_X, axis= 0)\n",
    "Hdouble_X = np.abs(Htheta_X)*np.exp(1j*np.angle(Htheta_X)*2)\n",
    "\n",
    "data = {'X': H_X, 'lapID': lapID, 'sp': sp}\n",
    "theta_data = {'X': Htheta_X, 'lapID': lapID, 'sp': sp}\n",
    "harm_data = {'X': Hharm_X, 'lapID': lapID, 'sp': sp}\n",
    "double_data = {'X': Hdouble_X, 'lapID': lapID, 'sp': sp}\n",
    "\n",
    "# Data separation and formatted file creation\n",
    "data_output = brianPyNet(data, width = \"Full\", dec = 8)\n",
    "theta_output = brianPyNet(theta_data, width = \"Theta\", dec = 8)\n",
    "harm_output = brianPyNet(harm_data, width = \"Harm\", dec = 8)\n",
    "double_output = brianPyNet(double_data, width = \"Double\", dec = 8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fnve",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
